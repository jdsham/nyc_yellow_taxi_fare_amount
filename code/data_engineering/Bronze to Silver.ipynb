{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Bronze Layer to Silver Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: This notebook uses Azure Synapse Analytics with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Here are the general steps to go from Bronze Layer to Silver Layer\n",
        "\n",
        "1. Load unioned bronze data\n",
        "2. Add trip duration in minutes to the data\n",
        "3. Filter the data\n",
        "4. Join in Zone ID data\n",
        "5. Transform the data\n",
        "6. Save the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "microsoft": {}
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:05:38.0595391Z",
              "execution_start_time": "2025-03-10T21:05:35.2827175Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "c92a2011-01ce-49a3-af27-18386ea5ceaf",
              "queued_time": "2025-03-10T21:01:53.9832282Z",
              "session_id": "39",
              "session_start_time": "2025-03-10T21:01:53.9843241Z",
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 2,
              "statement_ids": [
                2
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 2, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import Transformer, Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The raw bronze data was contactenated into one dataframe called union.\n",
        "Please review Concat Bronze notebook to see how this was done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:05:50.7641514Z",
              "execution_start_time": "2025-03-10T21:05:38.1600823Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "18d0928e-f585-4574-be58-66342d62691b",
              "queued_time": "2025-03-10T21:01:53.983987Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 3,
              "statement_ids": [
                3
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 3, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%pyspark\n",
        "df = spark.read.load(\"<YOUR BASE PATH GOES HERE>/bronze/union\", format='parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Definitions for Filters and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:08:53.3642719Z",
              "execution_start_time": "2025-03-10T21:08:53.2024886Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "aa113867-885b-4cff-8d53-a0b3c46f6e85",
              "queued_time": "2025-03-10T21:08:53.1316167Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 12,
              "statement_ids": [
                12
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 12, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class FilterVendorID(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Vendor ID.\n",
        "  Values must be 1 or 2\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"VendorID\"):\n",
        "    self.input_col = input_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter((F.col(self.input_col).isin([1,2])) & (F.col(self.input_col).isNotNull()))\n",
        "\n",
        "\n",
        "class FilterTimestamps(Transformer):\n",
        "  \"\"\"\n",
        "  Filters PU and DO timestamps\n",
        "  Timestamps must be bestween 2021-01-01 00:00:00 and 2025-01-01 00:00:00\n",
        "  Removes null values\n",
        "  \"\"\"\n",
        "  def __init__(self, input_cols=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]):\n",
        "    self.input_cols = input_cols\n",
        "\n",
        "  def _transform(self, df):\n",
        "    for input_col in self.input_cols:\n",
        "      df = df.filter( \n",
        "        (F.col(input_col) >= \"2021-01-01 00:00:00\")\n",
        "        & (F.col(input_col) < \"2025-01-01 00:00:00\")\n",
        "        & (F.col(input_col).isNotNull()))\n",
        "    return df\n",
        "\n",
        "class FilterPassengerCount(Transformer):\n",
        "  \"\"\"\n",
        "  Filters passenger count\n",
        "  Passenger Count must be between 1 and 6\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"passenger_count\", filter_value=list(range(1,264))):\n",
        "    self.input_col = input_col\n",
        "    self.filter_value = filter_value\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(\n",
        "        (F.col(self.input_col).isin(self.filter_value)) \n",
        "        & (F.col(self.input_col).isNotNull()))\n",
        "\n",
        "\n",
        "class FilterDistance(Transformer):\n",
        "  \"\"\"\n",
        "  Filters trip distance data.\n",
        "  Distance must be > 0 and <= 30 miles\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"trip_distance\", low_lim=0, up_lim=30):\n",
        "    self.input_col = input_col\n",
        "    self.low_lim = low_lim\n",
        "    self.up_lim = up_lim\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(\n",
        "        (F.col(self.input_col) > self.low_lim) \n",
        "        & (F.col(self.input_col) <= self.up_lim) \n",
        "        & (F.col(self.input_col).isNotNull()))\n",
        "  \n",
        "\n",
        "class FilterLocationID(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Location IDs for PU and DO locations\n",
        "  Must be between [1,263]\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_cols=[\"PULocationID\", \"DOLocationID\"], filter_value=list(range(1,264))):\n",
        "    self.input_cols = input_cols\n",
        "    self.filter_value = filter_value\n",
        "\n",
        "  def _transform(self, df):\n",
        "    for input_col in self.input_cols:\n",
        "      df = df.filter((F.col(input_col).isin(self.filter_value)) & (F.col(input_col).isNotNull()))\n",
        "    return df\n",
        "\n",
        "\n",
        "class FilterRateCodeId(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Rate Code ID\n",
        "  Must be between [1,6]\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"payment_type\", filter_value=list(range(1,7))):\n",
        "    self.input_col = input_col\n",
        "    self.filter_value = filter_value\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter((F.col(self.input_col).isin(self.filter_value)) & (F.col(self.input_col).isNotNull()))\n",
        "\n",
        "\n",
        "class FilterPayementType(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Payment Type\n",
        "  Must be between 1 or 2\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"RatecodeID\", filter_value=list(range(1,3))):\n",
        "    self.input_col = input_col\n",
        "    self.filter_value = filter_value\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter((F.col(self.input_col).isin(self.filter_value)) & (F.col(self.input_col).isNotNull()))\n",
        "\n",
        "\n",
        "class FilterFareAmount(Transformer):\n",
        "  \"\"\"\n",
        "  Filters fare amount\n",
        "  Price > $0 and <= $70\n",
        "  Value cannot be null\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"fare_amount\", low_lim=0, up_lim=70):\n",
        "    self.input_col = input_col\n",
        "    self.low_lim = low_lim\n",
        "    self.up_lim = up_lim\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(\n",
        "        (F.col(self.input_col) > self.low_lim)\n",
        "        & (F.col(self.input_col) <= self.up_lim) \n",
        "        & (F.col(self.input_col).isNotNull())\n",
        "        )\n",
        "        \n",
        "    \n",
        "class FilterExtra(Transformer):\n",
        "  \"\"\"\n",
        "  Filters extra\n",
        "  Price >= $0 and <= $15\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"extra\", low_lim=0, up_lim=15):\n",
        "    self.input_col = input_col\n",
        "    self.low_lim = low_lim\n",
        "    self.up_lim = up_lim\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(\n",
        "        (F.col(self.input_col) >= self.low_lim)\n",
        "        & (F.col(self.input_col) <= self.up_lim)\n",
        "        ).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterMTATax(Transformer):\n",
        "  \"\"\"\n",
        "  Filters MTA Tax\n",
        "  Value must be either 0 or 0.50\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"mta_tax\", filter_values=[0,0.5]):\n",
        "    self.input_col = input_col\n",
        "    self.filter_values = filter_values\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(F.col(self.input_col).isin(self.filter_values)\n",
        "        ).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterImprovementSurcharge(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Improvement Surcharge\n",
        "  Value must be either 0, 0.30, or 1.00\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"improvement_surcharge\", filter_values=[0,0.30,1.00]):\n",
        "    self.input_col = input_col\n",
        "    self.filter_values = filter_values\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(F.col(self.input_col).isin(self.filter_values)\n",
        "        ).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterTipAmount(Transformer):\n",
        "  \"\"\"\n",
        "  Filters tip amount\n",
        "  Price >= $0\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tip_amount\", low_lim=0):\n",
        "    self.input_col = input_col\n",
        "    self.low_lim = low_lim\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(F.col(self.input_col) >= self.low_lim).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterTollsAmount(Transformer):\n",
        "  \"\"\"\n",
        "  Filters tolls amount\n",
        "  Price >= $0 and <= $30\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tolls_amount\", low_lim=0, up_lim=30):\n",
        "    self.input_col = input_col\n",
        "    self.low_lim = low_lim\n",
        "    self.up_lim = up_lim\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(\n",
        "        (F.col(self.input_col) >= self.low_lim)\n",
        "        & (F.col(self.input_col) <= self.up_lim)\n",
        "        ).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterCongestionSurcharge(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Congestion Surcharge\n",
        "  Value must be either 0, 2.50\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"congestion_surcharge\", filter_values=[0,2.50]):\n",
        "    self.input_col = input_col\n",
        "    self.filter_values = filter_values\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(F.col(self.input_col).isin(self.filter_values)).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterAirportFee(Transformer):\n",
        "  \"\"\"\n",
        "  Filters Airport Fee\n",
        "  Value must be $0.00, $1.25, or $1.75\n",
        "  Null values will be imputed as 0\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"airport_fee\", filter_values=[0,1.25,1.75]):\n",
        "    self.input_col = input_col\n",
        "    self.filter_values = filter_values\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter(F.col(self.input_col).isin(self.filter_values)).fillna(0, subset=[self.input_col])\n",
        "\n",
        "\n",
        "class FilterDuration(Transformer):\n",
        "  \"\"\"\n",
        "  Filters trip duration data.\n",
        "  Duration > 1 minute and capped at 90 minutes\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"trip_duration_min\", filter_value = 1):\n",
        "    self.input_col = input_col\n",
        "    self.filter_value = filter_value\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.filter((F.col(self.input_col) > 1.0) & (F.col(self.input_col) <= 90.0) & (F.col(self.input_col).isNotNull()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:05:51.2657655Z",
              "execution_start_time": "2025-03-10T21:05:51.1127336Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "01ec89ce-eff3-4764-b487-8b13bfe1f2d6",
              "queued_time": "2025-03-10T21:01:55.2460795Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 5,
              "statement_ids": [
                5
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 5, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class ExtractDoW(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Day-of-Week from the timestamp\n",
        "  returns STRING\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_dow\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"E\"))\n",
        "\n",
        "\n",
        "class ExtractDoY(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Day-of-Year from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_doy\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"D\").cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractDoM(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Day-of-Month from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_dom\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"d\").cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractYear(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Year from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_year\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"y\").cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractMonth(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Month from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_month\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"M\").cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractYearMonth(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Month from the timestamp\n",
        "  returns String\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_year_month\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.concat(F.date_format(self.input_col, \"y\").cast(\"string\"),F.lit(\"-\"),F.date_format(self.input_col, \"M\").cast(\"string\")))\n",
        "\n",
        "\n",
        "class ExtractWeek(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Week number from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_week_number\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.weekofyear(self.input_col).cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractDate(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the date from the timestamp\n",
        "  returns string\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_date\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.to_date(self.input_col, \"yyyy-MM-dd\"))\n",
        "\n",
        "\n",
        "class ExtractHour(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Hour (0-23) from the timestamp\n",
        "  returns INT\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_hour\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.date_format(self.input_col, \"H\").cast(\"int\"))\n",
        "\n",
        "\n",
        "class ExtractDurationSec(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts trip duration in seconds\n",
        "  returns Numeric\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_col=\"tpep_pickup_datetime\", do_col=\"tpep_dropoff_datetime\", output_col=\"trip_duration_sec\"):\n",
        "    self.pu_col = pu_col\n",
        "    self.do_col = do_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.to_timestamp(F.col(self.do_col)).cast(\"long\") - F.to_timestamp(F.col(self.pu_col)).cast(\"long\"))\n",
        "\n",
        "\n",
        "class ExtractDurationMin(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts trip duration in minutes\n",
        "  returns Numeric\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_col=\"tpep_pickup_datetime\", do_col=\"tpep_dropoff_datetime\", output_col=\"trip_duration_min\"):\n",
        "    self.pu_col = pu_col\n",
        "    self.do_col = do_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, (F.to_timestamp(F.col(self.do_col)).cast(\"long\") - F.to_timestamp(F.col(self.pu_col)).cast(\"long\"))/60.0)\n",
        "\n",
        "\n",
        "class ExtractTimeOfDay(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts the Time of Day from the PU timestamp\n",
        "  returns STRING\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"pu_hour\", output_col=\"pu_tod\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "\n",
        "  def time_of_day(self, col_val):\n",
        "    if col_val >= 0 and col_val < 3:\n",
        "      tod = \"0-3\"\n",
        "    elif col_val >= 3 and col_val < 6:\n",
        "      tod = \"3-6\"\n",
        "    elif col_val >= 6 and col_val < 9:\n",
        "      tod = \"6-9\"\n",
        "    elif col_val >= 9 and col_val < 12:\n",
        "      tod = \"9-12\"\n",
        "    elif col_val >= 12 and col_val < 15:\n",
        "      tod = \"12-15\"\n",
        "    elif col_val >= 15 and col_val < 18:\n",
        "      tod = \"15-18\"\n",
        "    elif col_val >= 18 and col_val < 21:\n",
        "      tod = \"18-21\"\n",
        "    elif col_val >= 21 and col_val < 24:\n",
        "      tod = \"21-24\"\n",
        "    return tod\n",
        "\n",
        "  def _transform(self, df):\n",
        "    tod_udf = F.udf(self.time_of_day, StringType())\n",
        "    return df.withColumn(self.output_col, tod_udf(F.col(self.input_col)))\n",
        "\n",
        "\n",
        "class ExtractWithinBorough(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts if day is Weekday or Weekend from the timestamp\n",
        "  returns INT (1 = weekday, 0 = weekend)\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_borough=\"pu_borough\", do_borough=\"do_borough\", output_col=\"trip_within_borough\"):\n",
        "    self.pu_borough = pu_borough\n",
        "    self.do_borough = do_borough\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col,\n",
        "                         F.when((F.col(self.pu_borough) == F.col(self.do_borough)), 1)\n",
        "                         .when((F.col(self.pu_borough) != F.col(self.do_borough)), 0)\n",
        "                         .otherwise(F.lit(None)))\n",
        "\n",
        "\n",
        "class ExtractPuDoBoroughs(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts PU Borough and DO Borough into 1 column\n",
        "  returns STR\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_borough=\"pu_borough\", do_borough=\"do_borough\", output_col=\"pu_do_borough\"):\n",
        "    self.pu_borough = pu_borough\n",
        "    self.do_borough = do_borough\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.concat(F.col(self.pu_borough),F.lit('-'),F.col(self.do_borough)))\n",
        "\n",
        "\n",
        "class ExtractWithinZone(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts if trip is within same zone\n",
        "  returns INT (1 = yes, 0 = no)\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_zone=\"pu_zone\", do_zone=\"do_zone\", output_col=\"trip_within_zone\"):\n",
        "    self.pu_zone = pu_zone\n",
        "    self.do_zone = do_zone\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col,\n",
        "                         F.when((F.col(self.pu_zone) == F.col(self.do_zone)), 1)\n",
        "                         .when((F.col(self.pu_zone) != F.col(self.do_zone)), 0)\n",
        "                         .otherwise(F.lit(None)))\n",
        "  \n",
        "class ExtractRouteId(Transformer):\n",
        "  \"\"\"\n",
        "  Concats the PU Location ID and the DO Location ID\n",
        "  returns STRING\n",
        "  \"\"\"\n",
        "  def __init__(self, pu_col=\"PULocationID\", do_col=\"DOLocationID\", output_col=\"route_id\"):\n",
        "    self.pu_col = pu_col\n",
        "    self.do_col = do_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col, F.concat(self.pu_col, F.lit('-'), self.do_col))\n",
        "  \n",
        "class ExtractWeekday(Transformer):\n",
        "  \"\"\"\n",
        "  Extracts if day is Weekday or Weekend from the timestamp\n",
        "  returns INT (1 = weekday, 0 = weekend)\n",
        "  \"\"\"\n",
        "  def __init__(self, input_col=\"tpep_pickup_datetime\", output_col=\"pu_weekday\"):\n",
        "    self.input_col = input_col\n",
        "    self.output_col = output_col\n",
        "\n",
        "  def _transform(self, df):\n",
        "    return df.withColumn(self.output_col,\n",
        "                         F.when((F.date_format(self.input_col, \"E\").isin([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"])), 1)\n",
        "                         .when((F.date_format(self.input_col, \"E\").isin([\"Sat\", \"Sun\"])), 0)\n",
        "                         .otherwise(F.lit(None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Begin the Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Add trip duration in minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:05:51.8658019Z",
              "execution_start_time": "2025-03-10T21:05:51.3475247Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "f0c777b3-76d4-47d1-a926-665b5d500a88",
              "queued_time": "2025-03-10T21:02:14.8066966Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 6,
              "statement_ids": [
                6
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 6, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = df.withColumn(\"trip_duration_min\", (F.to_timestamp(F.col(\"tpep_dropoff_datetime\")).cast(\"long\") - F.to_timestamp(F.col(\"tpep_pickup_datetime\")).cast(\"long\"))/60.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Filter the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:08:56.8773618Z",
              "execution_start_time": "2025-03-10T21:08:56.722304Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "bfcda2b9-adf4-49e3-be88-22794f8e83f3",
              "queued_time": "2025-03-10T21:08:56.6505955Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 13,
              "statement_ids": [
                13
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 13, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the filters\n",
        "filter_stages = [\n",
        "    FilterVendorID(),\n",
        "    FilterTimestamps(),\n",
        "    FilterPassengerCount(),\n",
        "    FilterDistance(),\n",
        "    FilterLocationID(),\n",
        "    FilterRateCodeId(),\n",
        "    FilterPayementType(),\n",
        "    FilterFareAmount(),\n",
        "    FilterExtra(),\n",
        "    FilterMTATax(),\n",
        "    FilterImprovementSurcharge(),\n",
        "    FilterTipAmount(),\n",
        "    FilterTollsAmount(),\n",
        "    FilterCongestionSurcharge(),\n",
        "    FilterAirportFee(),\n",
        "    FilterDuration()\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:08:59.8610199Z",
              "execution_start_time": "2025-03-10T21:08:57.1020107Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "abcae123-d4bc-411d-9929-0aa5ef042582",
              "queued_time": "2025-03-10T21:08:57.0386887Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 14,
              "statement_ids": [
                14
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 14, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Filter the data\n",
        "filter_pipe = Pipeline(stages=filter_stages)\n",
        "filter_pipe_model = filter_pipe.fit(df)\n",
        "df = filter_pipe_model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:10:23.5966514Z",
              "execution_start_time": "2025-03-10T21:09:11.0510319Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "e0ea1f0b-c396-4eec-ae9b-aa5779a64911",
              "queued_time": "2025-03-10T21:09:10.9810606Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 15,
              "statement_ids": [
                15
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 15, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "128026248"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check to make sure the filters didn't eliminate all the data\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Join Zone data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:10:26.4560156Z",
              "execution_start_time": "2025-03-10T21:10:23.6958504Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "579e337c-d097-4b88-85ac-f3de5a1cdbfe",
              "queued_time": "2025-03-10T21:10:06.3192591Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 16,
              "statement_ids": [
                16
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 16, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the zone map.\n",
        "# This is done twice to keep things simple for doing two different joins.\n",
        "# Note that the data is only 265 rows, so it's relatively easy to get away this with practice, but not optimal in general\n",
        "zone_map = spark.read.load('<YOUR BASE PATH GOES HERE>/taxi_zone_lookup.csv', format='csv', header=True)\n",
        "zone_map2 = spark.read.load('<YOUR BASE PATH GOES HERE>/taxi_zone_lookup.csv', format='csv', header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:10:45.4130527Z",
              "execution_start_time": "2025-03-10T21:10:44.9045625Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "d0c4df1d-87cc-4f70-b2f7-76bf536a0131",
              "queued_time": "2025-03-10T21:10:44.8345146Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 17,
              "statement_ids": [
                17
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 17, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Join Zone Data onto Data\n",
        "df = df.join(F.broadcast(zone_map[[\"Zone\", \"Borough\", \"LocationID\"]]), df.PULocationID == zone_map.LocationID, \"inner\").select(df[\"*\"], zone_map[\"Zone\"].alias(\"pu_zone\"), zone_map[\"Borough\"].alias(\"pu_borough\"))\n",
        "df = df.join(F.broadcast(zone_map2[[\"Zone\", \"Borough\", \"LocationID\"]]), df.DOLocationID == zone_map2.LocationID, \"inner\").select(df[\"*\"], zone_map2[\"Zone\"].alias(\"do_zone\"), zone_map2[\"Borough\"].alias(\"do_borough\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Transform the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:10:49.0216222Z",
              "execution_start_time": "2025-03-10T21:10:48.8705812Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "94f9f24b-9812-4f90-a5f0-5708abae7b01",
              "queued_time": "2025-03-10T21:10:48.806483Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 18,
              "statement_ids": [
                18
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 18, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the transformations\n",
        "xform_stages = [\n",
        "  ExtractDoW(),\n",
        "  ExtractDoY(),\n",
        "  ExtractDoM(),\n",
        "  ExtractYear(),\n",
        "  ExtractMonth(),\n",
        "  ExtractYearMonth(),\n",
        "  ExtractWeek(),\n",
        "  ExtractDate(),\n",
        "  ExtractHour(),\n",
        "  ExtractDurationSec(),\n",
        "  ExtractWithinBorough(),\n",
        "  ExtractPuDoBoroughs(),\n",
        "  ExtractWithinZone(),\n",
        "  ExtractRouteId(),\n",
        "  ExtractWeekday(),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:10:51.2728886Z",
              "execution_start_time": "2025-03-10T21:10:50.1976296Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "fd253c46-c98d-4682-95fe-bcbc4cbb2ee0",
              "queued_time": "2025-03-10T21:10:50.1344065Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 19,
              "statement_ids": [
                19
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 19, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Transform the data\n",
        "xform_pipe = Pipeline(stages=xform_stages)\n",
        "xform_pipe_model = xform_pipe.fit(df)\n",
        "df = xform_pipe_model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:14:29.9949616Z",
              "execution_start_time": "2025-03-10T21:14:05.0838682Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "174ea5a5-9c86-435b-8afa-4b80921e7d66",
              "queued_time": "2025-03-10T21:11:17.5262815Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 21,
              "statement_ids": [
                21
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 21, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "128026248"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check to make sure the transformations didn't eliminate all the data\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Save the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-03-10T21:14:05.0095576Z",
              "execution_start_time": "2025-03-10T21:10:54.5340034Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "540bba0e-865e-4f6f-8c15-48a0fd9289ec",
              "queued_time": "2025-03-10T21:10:54.4673367Z",
              "session_id": "39",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "ExecSmall",
              "state": "finished",
              "statement_id": 20,
              "statement_ids": [
                20
              ]
            },
            "text/plain": [
              "StatementMeta(ExecSmall, 39, 20, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save the unioned data to avoid re-processing raw files again\n",
        "df.write.partitionBy(\"pu_year_month\").parquet(\"<YOUR BASE PATH GOES HERE>/silver/union\", mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
