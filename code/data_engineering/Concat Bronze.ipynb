{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Concat All Months Into A Single Dataframe\n",
        "\n",
        "Note: This notebook uses Azure Synapse Analytics with PySpark\n",
        "\n",
        "This notebook takes all the parquet files for each month of data from 2021 through 2024 and then concatenates all the data into one large parquet file.\n",
        "You only have to do this once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_input_path = \"<ADD YOUR PATH HERE>/bronze/\"\n",
        "base_output_path = \"<ADD YOUR PATH HERE>/bronze/\"\n",
        "output_file_path = f\"{base_output_path}union\"\n",
        "input_file_paths = []\n",
        "\n",
        "for year in range(2021,2025):\n",
        "    for month in range(1,13):       \n",
        "        # File Names\n",
        "        if month < 10:\n",
        "            month = f\"0{month}\"\n",
        "        file_name = f\"yellow_tripdata_{year}-{month}\"\n",
        "        full_input_path = f\"{base_input_path}{file_name}.parquet\"\n",
        "        input_file_paths.append(full_input_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "i = 0\n",
        "for k in range(0,len(input_file_paths)):\n",
        "    # Load the data\n",
        "    full_input_path = input_file_paths[k]\n",
        "    df = spark.read.load(full_input_path, format='parquet')\n",
        "    \n",
        "    # Change Airport_fee to airport_fee if present\n",
        "    if \"Airport_fee\" in df.columns:\n",
        "        df = df.withColumnRenamed(\"Airport_fee\", \"airport_fee\")\n",
        "\n",
        "    # Add the Year-Month column to enable better partitioning of data\n",
        "    # The Year-Month from the Pickup datetime will be used.\n",
        "    # The output column will be named \"pu_year_month\" where pu stands for pickup\n",
        "    df = df.withColumn(\"pu_year_month\", F.concat(F.date_format(\"tpep_pickup_datetime\", \"y\").cast(\"string\"),F.lit(\"-\"),F.date_format(\"tpep_pickup_datetime\", \"M\").cast(\"string\")))\n",
        "\n",
        "    # Union Data\n",
        "    if i == 0:\n",
        "        i+=1\n",
        "        union_df = df.limit(0)\n",
        "    union_df = union_df.unionByName(df)\n",
        "    \n",
        "# Clean up\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the unioned data to avoid re-processing raw files again\n",
        "union_df.write.partitionBy(\"pu_year_month\").parquet(output_file_path, mode='overwrite')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
